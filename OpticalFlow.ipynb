{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcV2rBHTIvzQ"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPVuznOQs2DD"
      },
      "outputs": [],
      "source": [
        "#telechargement de la base et autres dossiers\n",
        "url = \"https://drive.google.com/uc?id=1VrSh6miJhkHcR1jmaP3gMCp8KYDYtqvE\"  \n",
        "output = \"VideosTest.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "#unzip file\n",
        "!unzip \"VideosTest.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy4uUgLYiYzf"
      },
      "outputs": [],
      "source": [
        "#Calcul du Flot Optique Dense par la méthode de Farneback\n",
        "\n",
        "\n",
        "#Le flux vidéo est lu comme des captures d'objets par VideoCapture\n",
        "cap = cv.VideoCapture(\"/content/walking.mp4\")\n",
        "# ret retourne une valeur bouléenne indiquant l'obtention d'une capture, first_frame est le premier frame dans la séquence vidéo\n",
        "ret, first_frame = cap.read()\n",
        "# on converti ensuite le frame en niveau de gris car nous avons juste besoin de la luminance pour la détection des contours, ainsi on économise en puissance de calcul\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "# On crée ensuite une image avec une intensité nulle ayant la meme dimention que les frames de la vidéo.\n",
        "mask = np.zeros_like(first_frame)\n",
        "# on règle la saturation de l'image au maximum\n",
        "mask[..., 1] = 255\n",
        "\n",
        " \n",
        "while(cap.isOpened()):\n",
        "    #ret retourne une valeur bouléenne indiquant l'obtention d'un frame, frame est le frame courant capturé dans la vidéo.\n",
        "    ret, frame = cap.read()\n",
        "    #on converti chaque frame nouvellement capturé en niveau de gris \n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "   \n",
        "    #Calcul du flow optique dense par la méthode de Farneback.\n",
        "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "   \n",
        "    #Calcul des normes des angles des vecteurs 2D caratéristiques du flot optique\n",
        "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    # Définit la teinte de l'image en fonction de la direction du flot optique\n",
        "    mask[..., 0] = angle * 180 / np.pi / 2\n",
        "    # Définit la valeur de l'image en fonction de l'amplitude du flux optique (normalisé)\n",
        "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
        "    \n",
        "    # Conversion de l'espace couleur HSV à l'espace RGB(BGR)\n",
        "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
        "    \n",
        "    #on recupère la hauteur et la largeur du frame capturé pour définir la dimension d'affichage\n",
        "    h, w, _ = np.array(frame).shape\n",
        "    valeur_h = 0.6\n",
        "    valeur_w = 0.4\n",
        "    \n",
        "    # on éffectue une concaténation entre le frame d'origine et l'image résultant du calcul du flot optique et on les redimensionne pour les afficher sur la meme ligne.\n",
        "    affich_associe = np.concatenate((cv.resize(frame, (int(h * valeur_h), int(w * valeur_w))),\n",
        "                                  cv.resize(rgb, (int(h * valeur_h), int(w * valeur_w)))), axis=1)\n",
        "\n",
        "    # On affiche ensuite le resultat obtenu\n",
        "    print('flow optic dense')\n",
        "    cv2_imshow(affich_associe)\n",
        "    # mise à jour du frame précédent\n",
        "    prev_gray = gray\n",
        "    # Les frames sont lu à des intervalles de 30 millisecondes. \n",
        "    if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amLof22ZVFdO"
      },
      "outputs": [],
      "source": [
        "#Estimation du flot optique en fonction des normes\n",
        " \n",
        "# Le flux vidéo est lu comme des captures d'objets par VideoCapture\n",
        "cap = cv.VideoCapture(\"/content/walking.mp4\")\n",
        "# ret retourne une valeur bouléenne indiquant l'obtention d'une capture, first_frame est le premier frame dans la séquence vidéo\n",
        "ret, first_frame = cap.read()\n",
        "# on converti ensuite le frame en niveau de gris car nous avons juste besoin de la luminance pour la détection des contours, ainsi on économise en puissance de calcul\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "# On crée ensuite une image avec une intensité nulle ayant la meme dimention que les frames de la vidéo.\n",
        "mask = np.zeros_like(first_frame)\n",
        "# on règle la saturation et la teinte de l'image à zéro\n",
        "mask[..., 1] = 0\n",
        "mask[..., 0] = 0\n",
        " \n",
        "while(cap.isOpened()):\n",
        "    #ret retourne une valeur bouléenne indiquant l'obtention d'un frame, frame est le frame courant capturé dans la vidéo.\n",
        "    ret, frame = cap.read()\n",
        "    # on converti chaque frame nouvellement capturé en niveau de gris \n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calcul du flow optique dense par la méthode de Farneback.    \n",
        "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "   \n",
        "    #Calcul des normes et des angles des vecteurs 2D caratéristiques du flot optique\n",
        "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    #On Définit la valeur d'un seul canal de l'image en fonction de l'amplitude du flux optique (normalisé), la saturation et la teinte restant à zéro\n",
        "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
        "    \n",
        "    #Conversion de l'espace couleur HSV à l'espace RGB(BGR)\n",
        "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
        "\n",
        "\n",
        "    #on recupère la hauteur et la largeur du frame capturé pour définir la dimension d'affichage\n",
        "    h, w, _ = np.array(frame).shape\n",
        "    valeur_h = 0.6\n",
        "    valeur_w = 0.4\n",
        "    \n",
        "    # on éffectue une concaténation entre le frame d'origine et l'image résultant du calcul du flot optique et on les redimensionne pour les afficher sur la meme ligne.\n",
        "    affich_associe = np.concatenate((cv.resize(frame, (int(h * valeur_h), int(w * valeur_w))),\n",
        "                                  cv.resize(rgb, (int(h * valeur_h), int(w * valeur_w)))), axis=1)\n",
        "\n",
        "    \n",
        "    # On affiche ensuite le resultat obtenu\n",
        "    print(\"flot optique basé sur les normes\")\n",
        "    cv2_imshow(affich_associe)\n",
        "    #mise à jour du frame précédent\n",
        "    prev_gray = gray\n",
        "    # Les frames sont lu à des intervalles de 30 millisecondes\n",
        "    if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "snmCYp5d3Vnq"
      },
      "outputs": [],
      "source": [
        "#Segmentation avec seuillage selon les normes de flot optique des objets bougeant plus rapidement\n",
        " \n",
        "# Le flux vidéo est lu comme des captures d'objets par VideoCapture\n",
        "cap = cv.VideoCapture(\"/content/walking.mp4\")\n",
        "# ret retourne une valeur bouléenne indiquant l'obtention d'une capture, first_frame est le premier frame dans la séquence vidéo\n",
        "ret, first_frame = cap.read()\n",
        "# on converti ensuite le frame en niveau de gris car nous avons juste besoin de la luminance pour la détection des contours, ainsi on économise en puissance de calcul\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "# On crée ensuite une image avec une intensité nulle ayant la meme dimention que les frames de la vidéo.\n",
        "mask = np.zeros_like(first_frame)\n",
        "# on règle la saturation et la teinte de l'image au maximum\n",
        "mask[..., 1] = 255\n",
        "mask[..., 0] = 255\n",
        "#on défini le seuil et la taille du filtre\n",
        "seuil = 40\n",
        "kernel = np.ones(7)\n",
        " \n",
        "while(cap.isOpened()):\n",
        "    #ret retourne une valeur bouléenne indiquant l'obtention d'un frame, frame est le frame courant capturé dans la vidéo.\n",
        "    ret, frame = cap.read()\n",
        "    #on converti chaque frame nouvellement capturé en niveau de gris \n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "   \n",
        "    #Calcul du flow optique dense par la méthode de Farneback.\n",
        "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "   \n",
        "     #Calcul des normes et des angles des vecteurs 2D caratéristiques du flot optique\n",
        "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    \n",
        "    #On Définit la valeur d'un seul canal de l'image en fonction de l'amplitude du flux optique (normalisé), la saturation et la teinte restant à la valeur maximum\n",
        "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
        "    mask[..., 0] = 255\n",
        "    #on affecte au canal de la teinte les valeurs obtenu de la segmentation suivant les valeurs des normes des vecteurs flot optiques.\n",
        "    ret, mask[..., 0] = cv.threshold(mask[..., 2], seuil, 255, cv.THRESH_BINARY)\n",
        "    #Conversion de l'espace couleur HSV à l'espace RGB(BGR)\n",
        "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
        "\n",
        "    \n",
        "    #on effectue une opération de opening(érosion suivie d’une dilatation ) sur le canal contenant le résultat de la segmentation\n",
        "    opening = cv.morphologyEx(mask[..., 0], cv.MORPH_OPEN, kernel)  \n",
        "    #on effectue une opération de closing(dilatation suivie de l'érosion) sur le resulat du opening obtenu\n",
        "    closing = cv.morphologyEx(opening, cv.MORPH_CLOSE, kernel)  \n",
        "\n",
        "\n",
        "     #On crée donc un masque avec les résultats de la segmentation obtenus précédemment\n",
        "    masque = cv.cvtColor(closing, cv.COLOR_GRAY2BGR)\n",
        "     #on fait une concaténation entre l'image originale et l'image segmenté en rgb\n",
        "    affich_1 = np.concatenate((frame, rgb), axis=1)\n",
        "    #on fait une concaténation entre l'image originale et le masque de la segmentation\n",
        "    affich_2 = np.concatenate((frame, masque), axis=1)\n",
        "    #on associe les deux lignes précédentes et on les affiches\n",
        "    affich_final = np.concatenate((affich_1, affich_2), axis=0)\n",
        "  \n",
        "    \n",
        "    #On affiche ensuite le resultat obtenu\n",
        "    cv2_imshow(affich_final)\n",
        "    #mise à jour du frame précédent\n",
        "    prev_gray = gray\n",
        "    #Les frames sont lu à des intervalles de 30 millisecondes\n",
        "    if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cjB-3yzVTICw"
      },
      "outputs": [],
      "source": [
        "#Segmentation avec seuillage selon les normes de flot optique des objets bougeant à différentes vitesses\n",
        "\n",
        "#Le flux vidéo est lu comme des captures d'objets par VideoCapture\n",
        "cap = cv.VideoCapture(\"/content/walking.mp4\")\n",
        "#ret retourne une valeur bouléenne indiquant l'obtention d'une capture, first_frame est le premier frame dans la séquence vidéo\n",
        "ret, first_frame = cap.read()\n",
        "\n",
        "shape = (first_frame.shape[1], first_frame.shape[0])\n",
        "#on converti ensuite le frame en niveau de gris car nous avons juste besoin de la luminance pour la détection des contours, ainsi on économise en puissance de calcul\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "#On crée ensuite une image avec une intensité nulle ayant la meme dimention que les frames de la vidéo.\n",
        "mask = np.zeros_like(first_frame)\n",
        "\n",
        "# on définit deux valeur de seuils et la taille du filtre\n",
        "seuil_1 = 40\n",
        "seuil_2 = 100\n",
        "kernel = np.ones(7)\n",
        " \n",
        "while(cap.isOpened()):\n",
        "    #ret retourne une valeur bouléenne indiquant l'obtention d'un frame, frame est le frame courant capturé dans la vidéo.\n",
        "    ret, frame = cap.read()\n",
        "    #on converti chaque frame nouvellement capturé en niveau de gris\n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "   \n",
        "    #Calcul du flow optique dense par la méthode de Farneback.\n",
        "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    #Calcul des normes et des angles des vecteurs 2D caratéristiques du flot optique\n",
        "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    \n",
        "    #On Définit la valeur d'un seul canal de l'image en fonction de l'amplitude du flux optique (normalisé), la saturation et la teinte restant à la valeur maximum\n",
        "    magnitude = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
        "    \n",
        "    #on règle la saturation et le canal des valeurs à 128 et la teinte de l'image à zéro\n",
        "    mask[..., 0] = 0\n",
        "    mask[..., 1] = 128\n",
        "    mask[..., 2] = 128\n",
        "\n",
        "    #on applique le seuil_2 pour une segmentation sur les canaux de teinte et de saturation\n",
        "    ret, mask[..., 0] = cv.threshold(magnitude, seuil_2, 255, cv.THRESH_BINARY)\n",
        "    ret, mask[..., 1] = cv.threshold(magnitude, seuil_2, 255, cv.THRESH_BINARY)\n",
        "    #on parcours l'ensemble de pixels du canal de saturation et on donne la valeur 128 aux pixels valant 0 après le seuillage\n",
        "    mask[..., 1] = np.array(\n",
        "        [128 if value == 0 else value for value in mask[..., 1].reshape(shape[0] * shape[1])]).reshape(shape[1],shape[0])\n",
        "     #on applique le seuil_1 pour une segmentation sur les canaux de teinte et de value                                                                                                shape[0])\n",
        "    ret, mask[..., 0] = cv.threshold(magnitude, seuil_1, 255, cv.THRESH_BINARY)\n",
        "    ret, mask[..., 2] = cv.threshold(magnitude, seuil_1, 255, cv.THRESH_BINARY)\n",
        "    #on parcours l'ensemble des pixels du canal des value et on donne la valeur 128 aux pixels valant 0 après le seuillage\n",
        "    mask[..., 2] = np.array(\n",
        "        [128 if value == 0 else value for value in mask[..., 2].reshape(shape[0] * shape[1])]).reshape(shape[1], shape[0])\n",
        "                                                                                                      \n",
        "\n",
        "\n",
        "    #Conversion de l'espace couleur HSV à l'espace RGB(BGR)\n",
        "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
        "\n",
        "    #on effectue une opération de opening(érosion suivie d’une dilatation ) sur le canal de saturation contenant le resultat du seuillage avec seuil_2\n",
        "    opening_1 = cv.morphologyEx(mask[..., 1], cv.MORPH_OPEN, kernel)  \n",
        "    #on effectue une opération de closing(dilatation suivie de l'érosion) sur le resulat du opening obtenu\n",
        "    closing_1 = cv.morphologyEx(opening_1, cv.MORPH_CLOSE, kernel) \n",
        "    \n",
        "    #on effectue une opération de opening(érosion suivie d’une dilatation ) sur le canal de valeur contenant le resultat du seuillage avec seuil_1\n",
        "    opening_2 = cv.morphologyEx(mask[..., 2], cv.MORPH_OPEN, kernel) \n",
        "    #on effectue une opération de closing(dilatation suivie de l'érosion) sur le resulat du opening obtenu\n",
        "    closing_2 = cv.morphologyEx(opening_2, cv.MORPH_CLOSE, kernel) \n",
        "\n",
        "\n",
        "    #On crée donc un masque avec les résultats de la segmentation obtenus précédemment en associant les deux masques obtenus des deux seuillage\n",
        "    masque = closing_1 + closing_2\n",
        "    masque = cv.cvtColor(masque, cv.COLOR_GRAY2BGR)\n",
        "\n",
        "    #on fait une concaténation entre l'image originale et l'image segmenté en rgb\n",
        "    affich_1 = np.concatenate((frame, rgb), axis=1)\n",
        "    #on fait une concaténation entre l'image originale et le masque de la segmentation\n",
        "    affich_2 = np.concatenate((frame, masque), axis=1)\n",
        "    #on associe les deux lignes précédentes et on les affiches\n",
        "    affich_associe = np.concatenate((affich_1, affich_2), axis=0)\n",
        "\n",
        "    cv2_imshow(affich_associe)\n",
        "    \n",
        "    prev_gray = gray\n",
        "    \n",
        "    if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}